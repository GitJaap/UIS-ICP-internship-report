Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Widrob1990,
author = {Widrob, B and Widrob, B and Lehr, M a and Lehr, M a},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Widrob et al. - 1990 - 30 years of adaptive neural networks perceptron, Madaline, andbackpropagation.pdf:pdf},
journal = {Proceedings of the IEEE},
number = {9},
pages = {1415--1442},
title = {{30 years of adaptive neural networks: perceptron, Madaline, andbackpropagation}},
volume = {78},
year = {1990}
}
@article{Rohde2003,
abstract = {Nonrigid registration of medical images is important for a number of applications such as the creation of population averages, atlas-based segmentation, or geometric correction of functional magnetic resonance imaging (fMRI) images to name a few. In recent years, a number of methods have been proposed to solve this problem, one class of which involves maximizing a mutual information (MI)-based objective function over a regular grid of splines. This approach has produced good results but its computational complexity is proportional to the compliance of the transformation required to register the smallest structures in the image. Here, we propose a method that permits the spatial adaptation of the transformation's compliance. This spatial adaptation allows us to reduce the number of degrees of freedom in the overall transformation, thus speeding up the process and improving its convergence properties. To develop this method, we introduce several novelties: 1) we rely on radially symmetric basis functions rather than B-splines traditionally used to model the deformation field; 2) we propose a metric to identify regions that are poorly registered and over which the transformation needs to be improved; 3) we partition the global registration problem into several smaller ones; and 4) we introduce a new constraint scheme that allows us to produce transformations that are topologically correct. We compare the approach we propose to more traditional ones and show that our new algorithm compares favorably to those in current use.},
author = {Rohde, Gustavo K. and Aldroubi, Akram and Dawant, Benoit M.},
doi = {10.1109/TMI.2003.819299},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rohde, Aldroubi, Dawant - 2003 - The adaptive bases algorithm for intensity-based nonrigid image registration.pdf:pdf},
isbn = {0278-0062 (Print)},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Adaptive bases algorithm,Mutual information,Nonrigid image registration},
number = {11},
pages = {1470--1479},
pmid = {14606680},
title = {{The adaptive bases algorithm for intensity-based nonrigid image registration}},
volume = {22},
year = {2003}
}
@article{IBA2001,
abstract = {"Extended Ensemble Monte Carlo" is a generic term that indicates a set of algorithms, which are now popular in a variety of fields in physics and statistical information processing. Exchange Monte Carlo (Metropolis-Coupled Chain, Parallel Tempering), Simulated Tempering (Expanded Ensemble Monte Carlo) and Multicanonical Monte Carlo (Adaptive Umbrella Sampling) axe typical members of this family. Here, we give a cross-disciplinary survey of these algorithms with special emphasis on the great flexibility of the underlying idea. In Sec. 2, we discuss the background of Extended Ensemble Monte Carlo. In Sees. 3, 4 and 5, three types of the algorithms, i.e., Exchange Monte Carlo, Simulated Tempering, Multicanonical Monte Carlo, are introduced. In Sec. 6, we give an introduction to Replica Monte Carlo algorithm by Swendsen and Wang. Strategies for the construction of special-purpose extended ensembles are discussed in Sec. 7. We stress that an extension is not necessary restricted to the space of energy or temperature. Even unphysical (unrealizable) configurations can be included in the ensemble, if the resultant fast mixing of the Markov chain offsets the increasing cost of the sampling procedure. Multivariate (multicomponent) extensions are also useful in many examples. In Sec. 8, we give a survey on extended ensembles with a state space whose dimensionality is dynamically varying. In the appendix, we discuss advantages and disadvantages of three types of extended ensemble algorithms.},
author = {IBA, YUKITO},
doi = {10.1142/S0129183101001912},
issn = {0129-1831},
journal = {International Journal of Modern Physics C},
keywords = {EXPANDED-ENSEMBLE,FREE-ENERGY CALCULATIONS,LATTICE MODEL,LENNARD-JONES FLUID,MULTICANONICAL MOLECULAR-DYNAMICS,PHASE-TRANSITIONS,PROTEIN,REPLICA-EXCHANGE METHOD,SELF-OVERLAP ENSEMBLE,SPIN-GLASS SIMULATIONS,bridge,complexity ladder,exchange Monte Carlo,extended ensemble,multicanonical Monte Carlo,multivariate extension,replica Monte Carlo,simulated tempering},
month = {jun},
number = {05},
pages = {623--656},
publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD, 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
title = {{EXTENDED ENSEMBLE MONTE CARLO}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=49{\&}SID=N16xhyu49Pio1rvfLOx{\&}page=1{\&}doc=2},
volume = {12},
year = {2001}
}
@article{Au1999,
abstract = {An adaptive importance sampling methodology is proposed to compute the multidimensional integrals encountered in reliability analysis. It is based on a Markov simulation algorithm due to Metropolis et al. (Metropolis, Rosenbluth, Rosenbluth and Teller, Equations of state calculatons by fast computing machines. Journal of Chemical Physics, 1953;21(6): 1087-1092). In the proposed methodology, samples are simulated as the states of a Markov chain and are distributed asymptotically according to the optimal importance sampling density. A kernel sampling density is then constructed from these samples which is used as the sampling density in an importance sampling simulation. The Markov chain samples populate the region of higher probability density in the failure region and so the kernel sampling density approximates the optimal importance sampling density for a large variety of shapes of the failure region. This adaptive feature is insensitive to the probability level to be estimated. A variety of numerical examples demonstrates the accuracy, efficiency and robustness of the methodology, (C) 1999 Elsevier Science Ltd. All rights reserved.},
author = {Au, S.K. and Beck, J.L.},
doi = {10.1016/S0167-4730(99)00014-4},
issn = {01674730},
journal = {Structural Safety},
keywords = {DENSITY,Markov chain,Metropolis method,PROBABILITY,STRUCTURAL RELIABILITY,SYSTEMS,UNCERTAINTIES,UPDATING MODELS,importance sampling,reliability},
month = {jun},
number = {2},
pages = {135--158},
publisher = {ELSEVIER SCIENCE BV, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
title = {{A new adaptive importance sampling scheme for reliability calculations}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=WOS{\&}search{\_}mode=CitingArticles{\&}qid=27{\&}SID=N16xhyu49Pio1rvfLOx{\&}page=1{\&}doc=10},
volume = {21},
year = {1999}
}
@article{Wright2009,
abstract = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by l{\{}1{\}}-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as Eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
author = {Wright, J. and Yang, a. Y. and Ganesh, a. and Sastry, S S. and Ma, Y.},
doi = {10.1109/TPAMI.2008.79},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright et al. - 2009 - Robust face recognition via sparse representation.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Biometry,Biometry: methods,Cluster Analysis,Computer-Assisted,Computer-Assisted: methods,Face,Face: anatomy {\&} histology,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique},
number = {2},
pages = {210--227},
pmid = {19110489},
title = {{Robust face recognition via sparse representation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21646680},
volume = {31},
year = {2009}
}
@article{Bucher1988,
abstract = {An iterative Monte-Carlo simulation procedure for structural analysis is suggested This proposed new approach utilizes results from simulation to adapt the importance sampling density to the specific problem. Considerable reduction of the statistical error of the estimated failure probability is achieved. Most important, problems connected with optimization procedures commonly used in structural reliability are avoided. This makes the suggested procedure especially attractive for systems reliability analyses.},
author = {Bucher, Christian G.},
doi = {10.1016/0167-4730(88)90020-3},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bucher - 1988 - Adaptive sampling — an iterative fast Monte Carlo procedure.pdf:pdf},
isbn = {0167-4730},
issn = {01674730},
journal = {Structural Safety},
number = {2},
pages = {119--126},
title = {{Adaptive sampling — an iterative fast Monte Carlo procedure}},
volume = {5},
year = {1988}
}
@article{Lopes2007,
abstract = {An adaptive distributed strategy is developed based on incremental techniques. The proposed scheme addresses the problem of linear estimation in a cooperative fashion, in which nodes equipped with local computing abilities derive local estimates and share them with their predefined neighbors. The resulting algorithm is distributed, cooperative, and able to respond in real time to changes in the environment. Each node is allowed to communicate with its immediate neighbor in order to exploit the spatial dimension while limiting the communications burden at the same time. A spatial-temporal energy conservation argument is used to evaluate the steady-state performance of the individual nodes across the entire network. Computer simulations illustrate the results.},
author = {Lopes, Cassio G. and Sayed, Ali H.},
doi = {10.1109/TSP.2007.896034},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopes, Sayed - 2007 - Incremental adaptive strategies over distributed networks.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Adaptive networks,Consensus,Cooperation,Diffusion algorithm,Incremental algorithm,distributed processing},
number = {8},
pages = {4064--4077},
title = {{Incremental adaptive strategies over distributed networks}},
volume = {55},
year = {2007}
}
@article{Lustig2007,
abstract = {The sparsity which is implicit in MR images is exploited to significantly undersample k-space. Some MR images such as angiograms are already sparse in the pixel representation; other, more complicated images have a sparse representation in some transform domain-for example, in terms of spatial finite-differences or their wavelet coefficients. According to the recently developed mathematical theory of compressed-sensing, images with a sparse representation can be recovered from randomly undersampled k-space data, provided an appropriate nonlinear recovery scheme is used. Intuitively, artifacts due to random undersampling add as noise-like interference. In the sparse transform domain the significant coefficients stand out above the interference. A nonlinear thresholding scheme can recover the sparse coefficients, effectively recovering the image itself. In this article, practical incoherent undersampling schemes are developed and analyzed by means of their aliasing interference. Incoherence is introduced by pseudo-random variable-density undersampling of phase-encodes. The reconstruction is performed by minimizing the l(1) norm of a transformed image, subject to data fidelity constraints. Examples demonstrate improved spatial resolution and accelerated acquisition for multislice fast spin-echo brain imaging and 3D contrast enhanced angiography.},
author = {Lustig, Michael and Donoho, David and Pauly, John M.},
doi = {10.1002/mrm.21391},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lustig, Donoho, Pauly - 2007 - Sparse MRI The application of compressed sensing for rapid MR imaging.pdf:pdf},
isbn = {0740-3194},
issn = {07403194},
journal = {Magnetic Resonance in Medicine},
keywords = {Compressed sensing,Compressive sampling,Nonlinear reconstruction,Random sampling,Rapid MRI,Sparse reconstruction,Sparsity},
number = {6},
pages = {1182--1195},
pmid = {17969013},
title = {{Sparse MRI: The application of compressed sensing for rapid MR imaging}},
volume = {58},
year = {2007}
}
@article{Colless1999,
author = {Colless, M and {The 2DF Galaxy Redshift Survey Team} and Ellis, R and Bland-Hawthorn, J and Cannon, R and Cole, S and Collins, C and Couch, W and Dalton, G and Driver, S and Efstathiou, G and Folkes, S and Frenk, C and Glazebrook, K and Kaiser, N and Lahav, O and Lewis, I and Lumsden, S and Maddox, S and Peacock, J and Peterson, B and Price, I and Sutherland, W and Taylor, K},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Colless et al. - 1999 - The 2dF Galaxy Redshift Survey.pdf:pdf},
journal = {Looking Deep in the Southern Sky},
keywords = {clusters,cosmology,distances and redshifts,galaxies,general,large-scale structure of universe,observations,surveys},
pages = {9},
title = {{The 2dF Galaxy Redshift Survey}},
url = {http://adsabs.harvard.edu/cgi-bin/nph-bib{\_}query?bibcode=1999ldss.work....9C{\&}db{\_}key=AST},
volume = {1063},
year = {1999}
}
@article{Markovsky2015,
abstract = {Dynamic measurement aims to improve the speed and accuracy characteristics of measurement devices by signal processing. State-of-the-art dynamic measurement methods are model-based adaptivemethods, i.e., 1) they estimate model parameters in real-time and 2) based on the identified model perform model-based signal processing. The proposed model-free method belongs to the class of the subspace identification methods. It computes directly the quantity of interest without an explicit parameter estimation. This allows efficient computation as well as applicability to general high order multivariable processes.},
author = {Markovsky, Ivan},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markovsky - 2015 - Comparison of Adaptive and Model-Free Methods for Dynamic Measurement.pdf:pdf},
journal = {IEEE SIGNAL PROCESSING LETTERS},
number = {8},
pages = {1094--1097},
title = {{Comparison of Adaptive and Model-Free Methods for Dynamic Measurement}},
volume = {22},
year = {2015}
}
@article{Mahoney2011,
abstract = {Randomized algorithms for very large matrix problems have received a great deal of attention in recent years. Much of this work was motivated by problems in large-scale data analysis. Although this work had its origins within theoretical computer science, where researchers were interested in proving worst-case bounds, i.e., bounds without any assumptions at all on the input data, researchers from numerical linear algebra, statistics, applied mathematics, data analysis, and machine learning, as well as domain scientists have subsequently extended and applied these methods in important ways. Although this has been great for the development of the area and for the technology transfer of theoretical ideas into practical applications, this interdisciplinarity has thus far sometimes obscured the underlying simplicity and generality of the core ideas.   This review will provide a detailed overview of recent work on randomized algorithms for matrix problems, with an emphasis on a few simple core ideas that underlie not only recent theoretical advances but also the usefulness of these tools in large-scale data applications. Crucial in this context is the connection with concept of statistical leverage. This concept has long been used in statistical regression diagnostics to identify outliers; and it has recently proved crucial in the development of improved worst-case matrix algorithms that are also amenable to high-quality numerical implementation and that are useful to domain scientists. This connection arises naturally when one explicitly decouples the effect of randomization in these matrix algorithms from the underlying linear algebraic structure. This decoupling also permits much finer control in the application of randomization, as well as the easier exploitation of domain knowledge.},
archivePrefix = {arXiv},
arxivId = {1104.5557},
author = {Mahoney, Michael W.},
doi = {10.1561/2200000035},
eprint = {1104.5557},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahoney - 2011 - Randomized algorithms for matrices and data.pdf:pdf},
isbn = {9781601985064},
issn = {1935-8237},
pages = {49},
title = {{Randomized algorithms for matrices and data}},
url = {http://arxiv.org/abs/1104.5557},
year = {2011}
}
@article{Campbell2004,
abstract = {The Six Degree Field Galaxy Survey (6dFGS) is a spectroscopic survey of the southern sky, which aims to provide positions and velocities of galaxies in the nearby Universe. We present here the adaptive tiling algorithm developed to place 6dFGS fields on the sky, and allocate targets to those fields. Optimal solutions to survey field placement are generally extremely difficult to find, especially in this era of large-scale galaxy surveys, as the space of available solutions is vast (2N dimensional) and false optimal solutions abound. The 6dFGS algorithm utilises the Metropolis (simulated annealing) method to overcome this problem. By design the algorithm gives uniform completeness independent of local density, so as to result in a highly complete and uniform observed sample. The adaptive tiling achieves a sampling rate of approximately 95{\%}, a variation in the sampling uniformity of less than 5{\%}, and an efficiency in terms of used fibres per field of greater than 90{\%}. We have tested whether the tiling algorithm systematically biases the large-scale structure in the survey by studying the two-point correlation function of mock 6dF volumes. Our analysis shows that the constraints on fibre proximity with 6dF lead to under-estimating galaxy clustering on small scales ({\textless} 1 Mpc) by up to {\~{}}20{\%}, but that the tiling introduces no significant sampling bias at larger scales.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0403502},
author = {Campbell, Lachlan and Saunders, Will and Colless, Matthew},
doi = {10.1111/j.1365-2966.2004.07745.x},
eprint = {0403502},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campbell, Saunders, Colless - 2004 - The tiling algorithm for the 6dF galaxy survey.pdf:pdf},
issn = {00358711},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Large-scale structure of Universe,Methods: observational},
number = {4},
pages = {1467--1476},
primaryClass = {astro-ph},
title = {{The tiling algorithm for the 6dF galaxy survey}},
volume = {350},
year = {2004}
}
@article{Hosseini2012,
author = {Hosseini, Monireh Sheikh and Zekri, Maryam},
journal = {Journal of Medical Signals and Sensors},
keywords = {anfis,automatic diagnosis,medical image classification},
number = {1},
pages = {49--60},
title = {{Review of Medical Image Classification using the Adaptive Neuro-Fuzzy Inference System}},
volume = {2},
year = {2012}
}
@article{Allen2000,
abstract = {Combined EEG/fMRI recording has been used to localize the generators of EEG events and to identify subject state in cognitive studies and is of increasing interest. However, the large EEG artifacts induced during fMRI have precluded simultaneous EEG and fMRI recording, restricting study design. Removing this artifact is difficult, as it normally exceeds EEG significantly and contains components in the EEG frequency range. We have developed a recording system and an artifact reduction method that reduce this artifact effectively. The recording system has large dynamic range to capture both low-amplitude EEG and large imaging artifact without distortion (resolution 2 microV, range 33.3 mV), 5-kHz sampling, and low-pass filtering prior to the main gain stage. Imaging artifact is reduced by subtracting an averaged artifact waveform, followed by adaptive noise cancellation to reduce any residual artifact. This method was validated in recordings from five subjects using periodic and continuous fMRI sequences. Spectral analysis revealed differences of only 10 to 18{\%} between EEG recorded in the scanner without fMRI and the corrected EEG. Ninety-nine percent of spike waves (median 74 microV) added to the recordings were identified in the corrected EEG compared to 12{\%} in the uncorrected EEG. The median noise after artifact reduction was 8 microV. All these measures indicate that most of the artifact was removed, with minimal EEG distortion. Using this recording system and artifact reduction method, we have demonstrated that simultaneous EEG/fMRI studies are for the first time possible, extending the scope of EEG/fMRI studies considerably.},
annote = {Adaptive noise cancellation is somewhere within},
author = {Allen, P J and Josephs, O and Turner, R},
doi = {10.1006/nimg.2000.0599},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Allen, Josephs, Turner - 2000 - A method for removing imaging artifact from continuous EEG recorded during functional MRI.pdf:pdf},
issn = {1053-8119},
journal = {NeuroImage},
keywords = {Adult,Algorithms,Artifacts,Electroencephalography,Electroencephalography: methods,Electroencephalography: statistics {\&} numerical dat,Female,Humans,Image Processing, Computer-Assisted,Image Processing, Computer-Assisted: methods,Image Processing, Computer-Assisted: statistics {\&},Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Magnetic Resonance Imaging: statistics {\&} numerical,Male,Reproducibility of Results,Signal Processing, Computer-Assisted},
month = {aug},
number = {2},
pages = {230--9},
pmid = {10913328},
title = {{A method for removing imaging artifact from continuous EEG recorded during functional MRI.}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811900905998},
volume = {12},
year = {2000}
}
@article{Candes2006,
abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f{\&}isin;C{\textless}sup{\textgreater}N{\textless}/sup{\textgreater} and a randomly chosen set of frequencies {\&}Omega;. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set {\&}Omega;? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)={\&}sigma;{\textless}sub{\textgreater}{\&}tau;{\&}isin;T{\textless}/sub{\textgreater}f({\&}tau;){\&}delta;(t-{\&}tau;) obeying |T|{\&}le;C{\textless}sub{\textgreater}M{\textless}/sub{\textgreater}{\&}middot;(log N){\textless}sup{\textgreater}-1{\textless}/sup{\textgreater} {\&}middot; |{\&}Omega;| for some constant C{\textless}sub{\textgreater}M{\textless}/sub{\textgreater}{\textgreater}0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N{\textless}sup{\textgreater}-M{\textless}/sup{\textgreater}), f can be reconstructed exactly as the solution to the {\&}{\#}8467;{\textless}sub{\textgreater}1{\textless}/sub{\textgreater} minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C{\textless}sub{\textgreater}M{\textless}/sub{\textgreater} which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|{\&}middot;logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N{\textless}sup{\textgreater}-M{\textless}/sup{\textgreater}) would in general require a number of frequency samples at least proportional to |T|{\&}middot;logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.},
archivePrefix = {arXiv},
arxivId = {math/0409186},
author = {Cand{\`{e}}s, Emmanuel J. and Romberg, Justin and Tao, Terence},
doi = {10.1109/TIT.2005.862083},
eprint = {0409186},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cand{\`{e}}s, Romberg, Tao - 2006 - Robust uncertainty principles Exact signal reconstruction from highly incomplete frequency information.pdf:pdf},
isbn = {0018-9448 VO - 52},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Convex optimization,Duality in optimization,Free probability,Image reconstruction,Linear programming,Random matrices,Sparsity,Total-variation minimization,Trigonometric expansions,Uncertainty principle},
number = {2},
pages = {489--509},
pmid = {1580791},
primaryClass = {math},
title = {{Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information}},
volume = {52},
year = {2006}
}
@article{Lee2015,
author = {Lee, Jae-Woo and Kim, Seong-Eun and Song, Woo-Jin},
doi = {10.1016/j.sigpro.2015.01.019},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Kim, Song - 2015 - Data-selective diffusion LMS for reducing communication overhead.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Adaptive networks,Diffusion adaptation,Distributed estimation,Dynamic update,Selective communication},
pages = {211--217},
publisher = {Elsevier},
title = {{Data-selective diffusion LMS for reducing communication overhead}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168415000523},
volume = {113},
year = {2015}
}
@article{Almeida2000,
abstract = {The aim of this paper is to present the results on finite element adaptive strategies for computational fluid dynamics (CFD) problems with singularities arising from shock phenomena and/or discontinuous boundary data. The adaptive analysis is based on an optimal-mesh-adaptive strategy which is employed to refine the mesh, stretch and orient the elements in such a way that, along the adaptation process, the mesh becomes aligned with the discontinuities. This mesh adaptation process yields improved results in locating regions of rapid or abrupt variations of the variables, whose location is not known a priori. On the other hand, the proposed mesh adaptation process is generated by minimizing, for a given number of elements in the mesh, a new anisotropic error estimator based on local directional interpolation error and recovering of the second derivatives of the finite element solution. Several adaptive mesh-refinement solutions for interpolation problems are presented in order to show that the proposed optimal adaptive strategy using this anisotropic error estimator recovers optimal and/or superconvergent rates. Finally, applications of this approach to CFD problems are also presented in order to show the computational performance of the proposed optimal adaptive procedure.},
author = {Almeida, Regina C. and Feij{\'{o}}o, Ra{\'{u}}l A. and Gale{\~{a}}o, Augusto C. and Padra, Claudio and Silva, Renato S.},
doi = {10.1016/S0045-7825(99)00200-5},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Almeida et al. - 2000 - Adaptive finite element computational fluid dynamics using an anisotropic error estimator.pdf:pdf},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Adaptive analysis,Computational fluid dynamics,Error estimator,Finite elements,Mesh generation},
month = {feb},
number = {3-4},
pages = {379--400},
title = {{Adaptive finite element computational fluid dynamics using an anisotropic error estimator}},
url = {http://www.sciencedirect.com/science/article/pii/S0045782599002005},
volume = {182},
year = {2000}
}
@article{Dogancay2001,
abstract = {In some applications of adaptive filtering such as active noise reduction, and network and acoustic echo cancellation, the adaptive filter may be required to have a large number of coefficients in order to model the unknown physical medium with sufficient accuracy. The computational complexity of adaptation algorithms is proportional to the number of filter coefficients. This implies that, for long adaptive filters, the adaptation task can become prohibitively expensive, ruling out cost-effective implementation on digital signal processors. The purpose of partial coefficient updates is to reduce the computational complexity of an adaptive filter by adapting a block of the filter coefficients rather than the entire filter at every iteration. In this paper, we develop a selective-partial-update normalized least-mean-square (NI,MS) algorithm, and analyze its stability using the traditional independence assumptions and error-energy bounds. Selective partial updating is also extended to the affine projection (AP) algorithm by introducing multiple constraints. The new algorithms appear to have good convergence performance as attested to by computer simulations with real speech signals.},
annote = {adaptive filtering!},
author = {Dogancay, K. and Tanrikulu, O.},
doi = {10.1109/82.959866},
issn = {10577130},
journal = {IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing},
keywords = {GRADIENT ALGORITHMS,acoustic echo cancellation,adaptive filters,affine projection algorithm,normalized least-mean-square algorithm,partial updating},
number = {8},
pages = {762--769},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC, 345 E 47TH ST, NEW YORK, NY 10017-2394 USA},
title = {{Adaptive filtering algorithms with selective partial updates}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=10{\&}SID=P2yzWXnX9IsJ8SwDzgC{\&}page=1{\&}doc=6},
volume = {48},
year = {2001}
}
@article{Droske2001,
author = {Droske, Marc and Meyer, Bernhard and Rumpf, Martin and Schaller, Carlo},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Droske et al. - 2001 - An adaptive level set method for medical image segmentation.pdf:pdf},
journal = {{\ldots} Processing in Medical Imaging},
pages = {416--422},
title = {{An adaptive level set method for medical image segmentation}},
url = {http://link.springer.com/chapter/10.1007/3-540-45729-1{\_}43},
year = {2001}
}
@article{Tsao2012,
abstract = {In recent years, there has been an explosive growth of magnetic resonance imaging (MRI) techniques that allow faster scan speed by exploiting temporal or spatiotemporal redundancy of the images. These techniques improve the performance of dynamic imaging significantly across multiple clinical applications, including cardiac functional examinations, perfusion imaging, blood flow assessment, contrast-enhanced angiography, functional MRI, and interventional imaging, among others. The scan acceleration permits higher spatial resolution, increased temporal resolution, shorter scan duration, or a combination of these benefits. Along with the exciting developments is a dizzying proliferation of acronyms and variations of the techniques. The present review attempts to summarize this rapidly growing topic and presents conceptual frameworks to understand these techniques in terms of their underlying mechanics and connections. Techniques from view sharing, keyhole, k-t, to compressed sensing are covered.},
author = {Tsao, Jeffrey and Kozerke, Sebastian},
doi = {10.1002/jmri.23640},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsao, Kozerke - 2012 - MRI temporal acceleration techniques.pdf:pdf},
isbn = {1522-2586 (Electronic) 1053-1807 (Linking)},
issn = {10531807},
journal = {Journal of Magnetic Resonance Imaging},
keywords = {compressed sensing,dynamic imaging,k-t space,spatiotemporal redundancy,temporal acceleration,view sharing},
number = {3},
pages = {543--560},
pmid = {22903655},
title = {{MRI temporal acceleration techniques}},
volume = {36},
year = {2012}
}
@article{Uzgoren2009,
abstract = {Computational simulations of multiphase Row are challenging because many practical applications require adequate resolution of not only interfacial physics associated with moving boundaries with possible topological changes, but also around three-dimensional, irregular solid geometries. In this paper, we highlight recent efforts made in simulating multiphase fluid dynamics around complex geometries, based on an Eulerian-Lagrangian framework. The approach uses two independent but related grid layouts to track the interfacial and solid boundary conditions, and is capable of capturing interfacial as well as multiphase dynamics. In particular, the stationary Cartesian grid with time dependent, local adaptive refinement is utilized to handle the computation of the transport equations, while the interface shape and movement are treated by marker-based triangulated surface meshes which freely move and interact with the Cartesian grid. The markers are also used to identify the location of solid boundaries and enforce the no-slip condition there. Issues related to the contact line treatment, topological changes of multiphase fronts during merger or breakup of objects, and necessary data structures and solution techniques are also highlighted. Selected test cases including spacecraft fuel tank flow management and liquid plug flow dynamics are presented.},
author = {Uzgoren, Eray and Sim, Jaeheon and Shyy, Wei},
issn = {1815-2406},
journal = {COMMUNICATIONS IN COMPUTATIONAL PHYSICS},
keywords = {BOUNDARY METHOD,CAPTURING METHOD,CONTOUR RECONSTRUCTION METHOD,DENDRITIC SOLIDIFICATION,FRONT-TRACKING,IMMERSED INTERFACE METHOD,LEVEL SET METHODS,LIQUID PLUG,MOVING BOUNDARIES,Multiphase flows,NUMERICAL-SIMULATION,adaptive Cartesian grid,contact line treatment,interface tracking,irregular geometry,non-conforming boundary methods},
month = {jan},
number = {1},
pages = {1--41},
publisher = {GLOBAL SCIENCE PRESS, ROOM 2303, OFFICER TOWER, CONVENTION PLAZA, 1 HARBOUR ROAD, WANCHAI, HONG KONG 00000, PEOPLES R CHINA},
title = {{Marker-Based, 3-D Adaptive Cartesian Grid Method for Multiphase Flow Around Irregular Geometries}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=20{\&}SID=P2yzWXnX9IsJ8SwDzgC{\&}page=1{\&}doc=8},
volume = {5},
year = {2009}
}
@article{Saksono2007,
abstract = {The primary objective of this work is to extend the capability of the arbitrary Lagrangian-Eulerian (ALE)based strategy for solving fluid-structure interaction problems. This is driven by the fact that the ALE mesh movement techniques will not be able to treat problems in which fluid-structure interface experiences large motion. In addition, for certain problems the need arises to capture accurately flow features, such as a region with high gradients of the solution variables. This can be achieved by incorporating an adaptive refreshing procedure into the solution strategy.
As in our previous works (Comput. Methods Appl. Mech. Eng. 2006; 195:1633-1666; Comput. Methods Appl. Mech. Eng. 12006; 195:5754-5779), here, the fluid flow is governed by the incompressible Navier-Stokes equations and modelled by using stabilized low order velocity-pres sure finite elements. The motion of the fluid domain is accounted for by an arbitrary Lagrangian-Eulerian (ALE) strategy. The flexible structure is represented by means of appropriate standard finite element formulations while the motion of the rigid body is described by rigid body dynamics. For temporal discretization of both fluid and solid bodies, the discrete implicit generalized-g method is employed. The resulting strongly coupled set of non-linear equations is then solved by means of a novel partitioned solution procedure, which is based on the Newton-Raphson methodology and incorporates full linearization of the overall incremental problem.
Within the adaptive solution strategy, the quality of fluid mesh and the solution quality indicator are evaluated regularly and compared against the appropriate remeshing criteria to decide whether a remeshing step is required. The adaptive remeshing procedure follows closely the standard computational procedure in which the adaptive remeshing process produces a mesh that can capture salient features of the flow field. For the problems under consideration in this work the motion of the fluid boundary very often results in boundaries with very high curvatures and a fluid domain that contain areas with small cross-sections. To be able to generate meshes that give result with acceptable accuracy these local geometrical features need to be included in determining the element density distribution. The numerical examples demonstrate the robustness and efficiency of the methodology. Copyright (C) 2007 John Wiley {\&} Sons, Ltd.},
author = {Saksono, P. H. and Dettmer, W. G. and Peri{\'{c}}, D.},
doi = {10.1002/nme.1971},
issn = {00295981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {COMPUTATIONAL FRAMEWORK,ERROR ESTIMATION,FICTITIOUS DOMAIN METHOD,FINITE-ELEMENT FORMULATION,FREE-SURFACE FLOW,GENERALIZED-ALPHA METHOD,NAVIER-STOKES EQUATIONS,NUMERICAL-SIMULATION,Newton-Raphson solution method,PARTICULATE FLOWS,SLIP MESH UPDATE,adaptive strategy,finite element,fluid-structure interaction,partitioned approach},
month = {aug},
number = {9},
pages = {1009--1050},
publisher = {JOHN WILEY {\&} SONS LTD, THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND},
title = {{An adaptive remeshing strategy for flows with moving boundaries and fluid–structure interaction}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=15{\&}SID=P2yzWXnX9IsJ8SwDzgC{\&}page=1{\&}doc=6},
volume = {71},
year = {2007}
}
@article{Donoho2006,
abstract = {Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0{\textless}ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of "random" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0{\textless}ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that "most" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces},
archivePrefix = {arXiv},
arxivId = {1204.4227v1},
author = {Donoho, D.L. L},
doi = {Doi 10.1109/Tit.2006.871582},
eprint = {1204.4227v1},
file = {:home/jaap/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho - 2006 - Compressed sensing.pdf:pdf},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Adaptive sampling,Basis Pursuit,Compressed sensing,Data mining,Digital images,Euclidean space,Gel'fand,Image coding,Image reconstruction,Pixel,Quotient-of-a-Subspace theorem,Signal processing,Size measurement,Transform coding,Vectors,adaption,adaptive sampling,almost-spherical sections of Banach spaces,almost-spherical sections of banach spaces,approximation,bases,basis pursuit,convex optimization,convex programming,data compression,eigenvalues of random matrices,entropy numbers,gel'fand n-widths,general linear functional measurement,image coding,image reconstruction,image sampling,image sensors,information-based complexity,integrated sensing and processing,linear-operators,minimum,minimum l(1)-norm decomposition,noise,nonadaptive nonpixel sampling,optimal recovery,quotient-of-a-subspace theorem,representations,sensing compression,signal processing,spaces,sparse matrices,sparse representation,sparse solution of linear equations,transform coding},
number = {4},
pages = {1289--1306},
pmid = {1614066},
title = {{Compressed sensing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1614066},
volume = {52},
year = {2006}
}
